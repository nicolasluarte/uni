---
author: Luis Nicolás Luarte Rodríguez
title: Obesity and environmental uncertainty
geometry: "left=4cm,right=4cm,top=1cm,bottom=2cm"
linestretch: 1.5
---

# Introduction


The main aim of this section is (1) to review empirical evidence that links environment uncertainty with food-seeking behavior, mainly by observing changes in food intake, (2) then, possible ways uncertainty is represented at a neural level, emphasizing on the reward system. (3) to link the previous points to obesity, reinforcement learning and delayed discounting models will be presented as a way to establish a framework to characterize obesity as related to environmental uncertainty and the consequent effects on food-seeking behavior and food intake. Finally, (4) a review on evidence pointing to the Hypocretin/Orexin system as a foraging controller is presented as the immediate mechanisms linking uncertainty and food intake. In the following sections, obesogenic environments are considered in light of the previous framework, and specific types and characteristics of such environments will be pointed.

# Food-seeking behavior and uncertainty

Uncertainty implies situations where there is partial or incomplete information. When considering agency in decision-making tasks, uncertainty, refers to the incomplete information about the outcome that a given decision will generate, and also incomplete information about the probability distribution governing such outcome, whereas 'risk' implies knowledge about such probability distribution [@AF2655I2#DeGroot_Thurik_2018]. A particular feature of risk is the inverted 'U' shape, where at outcome probability 0 or 1 is at minima, and at maxima when probability is 0.5, this derives from risk being measured as outcome variance [@UKGJNWU7#Preuschoff_Bossaerts_Quartz_2006]. If an agent is situated in a natural environment, it is unlikely to have complete information about the decision-outcome pairing, and thus is forced to generates estimates or more complex models about environment statistical properties.

When considering the case of an agent in a natural environment, one of the most relevant cases is that of foraging. Foraging implies an agent searching for resources in a partially known environment, with depleting resources, and the supposition that the agent seeks to maximize its resources in the most efficient possible way, while accounting for unknown resource distribution [@ESYGCSLH#Charnov_1976]. Multiple theories on how an agent must decide to optimally allocate time to each resource patch [@IJSUH426#Wajnberg_Fauvergue_Pons_2000], optimal path [@A5N6NMAB#Hills_Kalff_Wiener_2013; @M5RXPXSZ#Humphries_Sims_2014] and evolutionary roots [@V2XKZN75#Wosniack_Etal_2017]. However, how environmental variables, such as resources uncertainty, modulate agent decision making have been less reviewed. Intuitively, if an agent does not have perfect information about the environment to be able to predict if there is going to be food in the future, it should consider a proxy of this, such as current food availability, which integrated over a time span, indicates the level of uncertainty of food availability.

To consider the modulation of uncertainty in decision making, one needs to take into account that uncertainty in food availability is a direct product of food scarcity, as the probability of food encounters is reduced proportional to scarcity. Thus, is to be expected that uncertainty effects are in line with food scarcity signaling, that is, a expected reduction in energy expenditure in order to preserve the energetic balance. However empirical evidence is not so clear in this regard [@U2XEEC7Q#Polo_2002], as both increases and reductions in body mass given increased levels of uncertainty in food availability [@CFC7DN65#Fokidis_Etal_2012]. When controlling for total food intake in condition of fixed or variable food availability, studies with birds show a decrease in body mass, which is explained, in part, by increased locomotor activity [@CFC7DN65#Fokidis_Etal_2012], this results are discussed in terms of stress (augmented due to food variability). However they can also be interpreted of expected foraging behavior.

When an environment presents higher uncertainty on food availability, it could be expected that food-seeking bouts are to be increased, in order to compensate for reduced food encounters or, complementary to that, an increased hoarding-type behavior. In the face of uncertainty, specifically, regarding food resources, a typical behavior is to increase food-seeking bouts, and resulting hoarding-type behavior, allowing the consumption of extra calories. This has been considered as a mechanism to prevent possible starvation because uncertainty is used as a proxy of future food scarcity, so eating in excess could prevent starvation [@C6Z374UG#Anselme_Güntürkün_2019]. However, this hoarding-behavior can also be explained by directly estimating food availability in the environment. Food scarcity does not necessarily act as an indicator of uncertainty, as food resources, while fewer, can be found in predictable places or at periodical time intervals. Nevertheless, when previous regularities regarding feeding routine, such as feeders position is constantly changed, increased intake is observed relative to the unchanging environment [@NL4XYLRH#Forkman_1993]. This shows that, while food scarcity could be a cause of hoarding behavior, environment properties are enough to trigger such behaviors. In the previous case, in addition to altering the environment, food reserves were also altered. However, its levels were more than sufficient to satisfy energetic demands. Nevertheless, the uncertainty effect on food intake holds even when food levels are equated through predictable and unpredictable settings [@3GQMEPEH#Cuthill_2000]. In addition to increased intake, under uncertainty, foraging patterns, under mathematical modelling, are changed towards the start of day to account for missed foraging bouts [@LLLWQCZE#Bednekoff_Houston_1994].

Up to this point, there are at least 3 points to consider regarding food uncertainty, (1) under food availability uncertainty, food-seeking behavior is increased [@2YZE6SVW#Robinson_Etal_2014; @CFC7DN65#Fokidis_Etal_2012; @U2XEEC7Q#Polo_2002]. (2) when uncertainty is increased strategies to maintain energetic balance, such as hoarding [@C6Z374UG#Anselme_Güntürkün_2019] or increasing body mass [@BW3RY7GD#Moiron_Mathot_Dingemanse_2018; @3GQMEPEH#Cuthill_2000] emerge. (3) such strategies imply a trade-off between preventing starvation and increasing risk of predation (due to reduced mobility), and as such are suggestive of a dynamic balance [@8XA5RHVI#Macleod_Etal_2005].

To address food-seeking behavior under uncertainty, one needs to consider which behavior is reflective of a food-seeking action. In typical experimental settings, such action is reflected by interaction with cues or apparatuses that are related to reward delivery, when the action is interacting with a conditioned stimuli is called sign-tracking, whereas if interaction is with food dispenser is called goal-tracking [@ZW878GVH#Silva_Silva_Pear_1992]. More specifically, sign-tracking, refers to an approaching behavior towards previously conditioned stimuli and rewards. So, it implies a previous conditional-stimulus and unconditional stimulus pairing, and, afterwards, tracking of the signal that was previously associated with the reward [@BDFISDRS#Flagel_2014].

When uncertainty is introduced at the stage of conditional and unconditional stimulus pairing, as the probability of reward delivery upon lever pressing, sign-tracking increases as the probabilities of reward delivery approaches 50%, and the amount of reward is more varied [@S8CHV5KG#Anselme_Robinson_Berridge_2013]. In this case, as the delivery of a given reward gives no information about following one (delivery is determined by a probability function, independent of animal action), it can be assumed that, under Shannon entropy formulation, entropy (which can be understood as a measure of uncertainty) reaches the peak at 50% probability, and, furthermore, it predicts that uniform distributions, with more outcomes, increase uncertainty. Both were the case in the previously presented experiment (assuming uncertainty drove signal-tracking) as 50% probability of delivering 2 or 0 pellets had lower signal-tracking than 50% probability of delivering 0 or 1, 2, or 3 pellets with equal probability (16.7% for 1, 2 or 3 pellets). This, again, points out that increased food-seeking related behavior increases upon increased uncertainty even when food availability is controlled. This effect has been replicated in studies with amphetamine sensitization, where uncertainty (on conditioned stimulus and unconditioned stimulus) and sensitization, independently, augmented sign-tracking behavior, however, the effect of both uncertainty and sensitization was not additive suggesting a ceiling effect [@UZET5L4Z#Robinson_Etal_2015].

The increased, food-seeking related behavior magnification by uncertainty, has been found with partial reinforcement procedures [@P3MLPUGY#Collins_Etal_1983], with manipulation of food placement variability [@NL4XYLRH#Forkman_1993], variability on reward quality and delivery delay [@YI6J6Y7I#Craft_2016], and in sequential probability tasks [@YB2MIP4H#Stagner_Zentall_2010]. Implying a robust effect across multiple food-related uncertainty scenarios.

From the perspective of a foraging animal, food sources are distributed in a partially known space, where effort must be made to obtain such sources. Uncertainty, reveals the consistency of food sources in a given space, where more uncertainty determines more difficulty in obtaining food. However, the consistency of food sources must be sensed through a mechanism that updates its estimate in a trial by trial basis, because is safe to assume that an agent interested in sensing environment uncertainty does not possess complete information. A plausible mechanism is to sense uncertainty, indirectly, via the reward prediction error. The reward prediction error is simply $$ actual\,reward - expected\,reward $$ As the reward prediction error is thought to operate in environments where a particular action lead to a probable reward, this error is used to update the value of any given action, then, the value of such time step (which can be associated with a given action) is given by the discounted rewards from that point onwards up to the termination of the trial series [@2BEHEM7X#Sutton_Barto_2018] $$ expected\,reward = reward_{t+1} + \gamma reward_{t+2} + \gamma^2 reward_{t+3} + \ldots + \gamma^k reward_{T}$$ Here the trial series is composed of $T$ time steps with a discount factor $\gamma, 0 \leq \gamma \leq 1$. The discount factor is there to signal the typical preference for obtaining rewards now rather than latter, on and how big it is will depend on properties of both agent and environment [@R3TZXYBW#Glimcher_2011].

The formulation presented above is just a mathematical representation of several assumptions of how an agent can learn expected reward values in a finite, trial based, experiment and then calculate the prediction error at each time step, how this reward prediction error is used to update values will be presented latter on. However, the main idea is that over trials, as the expected value approximates the real one, the reward prediction error goes down, nevertheless, if rewards value change the error goes up reflecting this change (see figure \ref{rpe}).

The main neural circuitry supporting the computation of the reward prediction error is thought to be supported, mainly, by the dopamine system [@8B5TPXB5#Schultz_2016]. As the reward prediction error was first derived from behavioral data, to assess the biological feasibility three components must exist (1) expectation encoding units; (2) reward encoding units and (3) a subtraction unit [@NLDHLRVN#WatabeUchida_Eshel_Uchida_2017]

Reward prediction errors models predict both three cases (1) where the expected reward and current reward are equal (no prediction error); (2) expected reward is less than the current reward (negative error) or (3) expected reward is greater than current reward (positive error). Midbrain dopamine neurons have been found to encode positive error but not negative under reinforcement learning models [@ZHGB75KH#Bayer_Glimcher_2005]. Around this point two main hypothesis have been formulated, the first, proposes than negative error are encoded via lowering the fire-rate compared to the baseline [@33K2X73I#Schultz_Dayan_Montague_1997], whereas the second, proposes an opponency system between dopamine and serotonin systems [@JZJAAAYD#Daw_Kakade_Dayan_2002]. Dopamine neurons in the ventral tegmental area have been found to encode the future discounted rewards [@JPZD7WDQ#Enomoto_Etal_2011]. This two lines of evidence points that dopamine is capable of encoding expectation, reward value and doing subtraction (perhaps including the serotonin system), showing a significant complexity of this system, which might exceed value-related computations [@698KWSAL#Takahashi_Etal_2017].

Link reward prediction error to uncertainty
Above the function of dopamine neurons in reward prediction error has been stated, more specifically, this function seems to be related to the phasic activations, whereas, more sustained activation is related to reward uncertainty (measured as reward variance, thus reaching its peak at a probability of 0.5) [@AR2TQB84#Fiorillo_2003]. Such uncertainty-related signal has also been found in the orbito frontal cortex, amygdala [@YWWID6GW#Schultz_Etal_2008] and medial frontal lobe [@ZRTDBMRA#Huettel_2005]. A plausible hypothesis to link the reward prediction error and uncertainty encoding, can state that, over time, reward prediction error signals are integrated into an uncertainty signal, as, over time, more error is to be expected under higher reward variability (see figure \ref{uncertainty}). However, evidence points towards independent signals of reward prediction error and uncertainty in the orbito frontal cortex [@HVH8KZZ2#Rushworth_Behrens_2008]. Nevertheless, at least at a computational level, the reward prediction error can be used to estimate the reward-related uncertainty [@P2FYNJKR#Soltani_Izquierdo_2019].

Reward prediction error also contains information about the divergence between the current policy the animal is following to obtain food (or reward), and the optimal (or desired one). Such a system, then, is able to modify decision-making policies based on the reward-prediction error [@87WR6HV3#Pessiglione_Etal_2006].

Summing these ideas up, food-seeking behavior can be conceptualized as a series of decision-making actions that occur in an environment with varying degrees of uncertainty, and where each feeding bout is evaluated by the reward prediction error. Then, and averaging over a history of feeding bouts, environment uncertainty can be obtained via the spread of the rewards relative to the mean. Evidence of neural systems actually sustaining this point that increased activity in dopaminergic systems can reflect uncertainty [@AR2TQB84#Fiorillo_2003], and computational models have pointed the possibility of a basal ganglia circuitry that encodes mean rewards and its spread [@BUXRNM89#Mikhael_Bogacz_2016].


## Environment uncertainty and food intake


## Uncertainty representations at the neural level

If uncertainty can modulate food-seeking behavior in order to increase intake and better sustain energetic reserves, it is expected to have at least to functional instances (1) an uncertainty sensing unit and (2) a reward processing unit, which can relay information to homeostatic-related and decision-making loci, to integrate such information and determine the next action to take. To determine the neural substrates of such instances, environment-agent dynamics can be represented through Markov decision tasks. Such tasks consider a set of states with possible transitions between each one, and two functions (1) the one in charge of determining the state transition given the agent action and (2) an action-state-reward function which maps a reward to a given action-state tuple. In such tasks, uncertainty is derived from probability matrices assigned to either of the two functions. When state transition functions in manipulated, two scenarios can be created (1) a regular one, where action-state transitions are deterministic, and (2) a random one, where action-state can not be predicted. Considering the latter, ventral striatal neurons can encode the prediction of immediate rewards, whereas the dorsal striatum is related to the former [@G5PZHPCI#Tanaka_Etal_2006]. This can be explained in terms of immediate and long-term reward prediction, as state transitions are random, subjects can only reliably predict the following rewards, in turn, if state-transition dynamics are deterministic, the reward of a long series of actions can be predicted.

If an environment is stable, then state-action-reward mappings can be optimized to reduce reward-prediction error. In this way, when the mapping is optimized, reward-related circuitry should reduce its activity [@V9CIKRBE#Friston_2009]. However, this mapping is always modulated by environment dynamics regarding uncertainty. An optimal mapping in a given environment state can increase the reward prediction error in the same environment if this is non-stationary. The anterior cingulate cortex (ACC) has been shown to increase its activation levels when predictability in the environment drops [@9SRBHI4W#Davis_Choi_Benoit_2010], effectively signaling environment dynamics.

As previously stated, environment dynamics need to be taken into account in order to appropriately interpret obtained rewards. If I visit a restaurant and the food served is delicious, my rating of the restaurant should not be too hasty as this could be just good luck. However, if this has always been the case, giving a high rating would be the correct choice. In term of rewards, uncertainty is high when a given rewards give no information about the ones to come, conversely, certainty is achieved when a given reward gives all information about the following one. Direct tracking of environment volatility has been found to be well represented in the ACC [@BHR2NAEI#Behrens_Etal_2007], presumably by encoding some sort of learning rate that bias valuation of rewards more to the short-term if volatility is high, and to the long-term is volatility is lower. The competing hypothesis of ACC describes its function to a decision-difficulty sensing unit, or demand of control when overriding default action is more optimal [@J9QC5JYH#Shenhav_Cohen_Botvinick_2016]. However, it should be noted that @Behrens_Etal_2007 results were circumscribed to the time point where the outcome is observed, which corresponds to the proper timing to assign obtained reward influence to the following behavior.

When representing the uncertainty of a given environment, an agent must pair the value obtained with the action performed. For each action possible, the agent updates the value of the action-reward tuple based on the reward prediction error.

Temporal dynamics of action-reward pairing and reward prediction error are such that the former occurs first relative to the later. Such temporal difference is reasonable because the pairing should be represented when taken action, and the prediction error requires feedback in order to compare obtained versus expected rewards. Considering this, the action-reward pairing has been found to be correlated to activity at the putamen, whereas rewards-prediction error, to be represented in the caudate nucleus [@DAKDZEVY#Haruno_Kawato_2006]. However, as the authors point, both structures are likely to be involved in a larger loop containing the ACC, which would make sense to integrate reward evaluation over states, actions, and environmental uncertainty, and optimally influence following behavior.

It can be inferred from the way action-reward pairing is stated that it corresponds to action selection based on a history of rewards, which are mediated by the reward prediction error. Inhibition of putamen activity has effectively shown a reduction in performance when the task requires the consideration of reward history to select correct actions [@XZBUNRIS#Muranishi_Etal_2011]. Signal encoding, however, seems to be more complex, as basal ganglia direct pathway encode rewards outcomes, and the indirect pathway represents the next-action selection [@4F2PUL7U#Nonomura_Etal_2018]. Together, this points to a multi-structure network that represents expected and obtained rewards as an error, which allows easing computational requirements as the current state needs only to be compared with the expectation, that encompasses all previous history of rewards. Moreover, this signal updates rewards given actions, while considering environment volatility and the proper weighting of immediate versus long-term rewards. Thus, allowing to optimize behavior even when environments are non-stationary and rapidly changing.


## Models explaining food intake in obesity

### Reinforcement learning models

Temporal-difference learning models state how agents can estimate reward values in uncertain environments. At each time-step, the agent computes the value of a given state considering: (1) the estimated value (randomly initiated at first), and (2) the temporal-difference error, which represents the distance between the estimate of state value and the actual reward obtained in such state.

\begin{equation}
	V(S_t) \leftarrow V(S_t) + \alpha(Temporal \, Difference \, Error)
\end{equation}

$V(S_t)$ denotes the estimated value at a given state, and $\alpha$ is used to model the agent learning rate, that is, the rate at which state value is updated, and thus able to affect agent behavior. Additional parameter $\rho$ has been proposed to model sensitivity to reward [@JGKMEKV6#Huys_Etal_2013; @GU4KJGAS#Kroemer_Small_2016], such that the temporal difference error accounts for the subjective value of obtained rewards.

\begin{equation}
	Temporal \, Difference \, Error = \rho \times Reward - V(S_t)
\end{equation}

Obese subjects had shown reduced dorsal striatum activity to food rewards, which has been interpreted as reduced pleasure for food. However, simulations under the previously presented model show another option. That is, obese subjects show heightened reward sensitivity but decreased learning rates, ending in a lowered state value estimation [@GU4KJGAS#Kroemer_Small_2016]. Modeled learning rates measures had shown that this is the case in obese subjects. Moreover, it points that negative prediction errors (the equivalent of temporal difference error) were used to a lesser extent than lean subjects, whereas positive errors showed no differences [@DBGUDZRL#Mathar_Etal_2017]. This can be interpreted as a difficulty to update reward or state values when the estimated reward is higher than the actual reward, possibly reflecting a short-term reward estimation.

It should be noted that more recent neuroimaging evidence points in favor of a hyper-reactivity of rewards circuitry, instead of hypo-reactivity. However, conclusions obtained by the model still hold, as such, hyper-reactivity is accompanied by a bias towards immediate rewards [@WCFW3TEU#Stice_Burger_2019]. In line with the reinforcement learning model presented, evidence from probabilistic learning paradigms in obese subjects shows a decreased impact of negatively valued choices on consequent behavioral adaptation [@PE46B3Z8#Kube_Etal_2018]. These seemingly opposing results can stem from, previously not considered, quadratic associations between BMI and reward sensitivity, where an inverted U-shape is observed as BMI increases [@67GSY6VY#Horstmann_Fenske_Hankir_2015]. Taken together, this finding suggests that obesity overfeeding is not only reliant on increased reward sensitivity (more reward sensitivity is assumed to increase intake), but other parameters such as learning rates can determine the overall valuation of the reward, biasing decision-making to immediate rewards, that paired with highly palatable food can lead to excess caloric intake.

More complex models can include reward sensitivity in addition to different palatability indices of food encounters, effectively modeling the course of an agent with reward heterogeneity. Additionally, agents in this model are allowed to learn the value of different rewards values through different environments, such as highly-palatability, low-palatability, and mixed. Later on, the effects of the starting environment can be assessed. Results suggest that starting in an abundance of highly-palatable food slows the learning of food reward values in the following environments [@DJFPYDKI#Hammond_Etal_2012]. While this model does not inform about the effect on weight or intake levels, it shows how agents react to initial conditions or, more generally, to non-stationary environments.


### Delayed discounting models

Although the factors determining obesity as an outcome are multiple [@UQ9I8YFE#Ang_Etal_2013], it is reasonable to assume that the more immediate cause is excess intake relative to energetic demands. Moreover, excess intake is determined in an instance to instance basis, where a decision considering short and long-term benefits/risks must be made. With this in consideration, one can assume that obesity, in part, is caused by sub-optimal short/long-term benefit/risk assessments when making feeding decisions. If this was the case, as previously noted, areas that are related to computing options value in the short/long term, such as the ACC, should be in some way impaired.

Delayed discounting refers to the depreciation of a certain reward as a function of the time required to obtain it [@9YRMQU6H#DaMatta_Gonçalves_Bizarro_2012]. As such, it provides measures of how reward-related systems bias decision to the short or long term. Obese subjects show a robust tendency to steeply discount future rewards [@FCVQ7SB6#Amlung_Etal_2016], thus, favoring short-term rewards. 

Furthermore, ACC, among other structures, shows relative atrophy in obese subjects [@THS94923#Wang_Etal_2017;@5FBDWF55#Raji_Etal_2009], suggesting an impairment of the previously mentioned functions. These findings can be interpreted as if impairment in environment uncertainty assessment results in a preference for short-term rewards. If this were the case, palatable food sensory cues, which trigger food-intake, would dominate over more long-term modulated decisions, such as healthy food intake [@J37T9F5T#Higgs_2016].

Higher future rewards discounting paired with increased motivation to work for food, predict higher caloric intake [@GHWUZPNL#Rollins_Dearing_Epstein_2010], and this effect seems to hold even for low energy-density food [@TLRSYNN6#Epstein_Etal_2014]. The rate of reward discounting, thus, informs about the predisposition to increased energetic intake, independent of possible food-property related effects. Similar effects have been found in children [@8QS2B4AF#Best_Etal_2012], but not in adult males [@IATULRKH#Smulders_Boswell_Henderson_2019]. Moreover, these effects seem to be directly related to body fat [@7B4XUYMB#Rasmussen_Lawyer_Reilly_2010].

## Orexin/Hypocretin system control of foraging

Up to this point, the way reward-related systems interact with environmental uncertainty has been discussed. Several structures seem to be involved in integrating reward value in the face of environment volatility. Moreover, empirical findings of food-seeking behavior in predictable/unpredictable environments were pointed out. However, the direct mechanism that guides food-seeking behavior is lacking. One such system is the Orexin/Hypocretin (HO), which is part of the energetic homeostasis and feeding pathways [@W3IUQNUY#Toshinai_Etal_2003], playing a large role in increasing food intake [@5994QM3A#Wolf_2009]. However, a more broad and complex opioid system is thought to control food intake, which in turn is modulated by food preference, and has proven to be selective to certain macro-nutrients, such as fat [@W79GBSE7#Taha_2010]. More recent evidence has linked the activation of the hypothalamic HO system to an increase in short-term spatial memory, which is a function that supports exploratory foraging behavior [@28CTMBAN#AittaAho_Etal_2016].

Moreover, orexin promotion of such foraging-related behavior has been postulated as one of its main functions [@FIT9A4HW#Barson_2020]. Such function is relevant because foraging behavior evolved in a specific type of environment, where resources are sparse, clustered, and is a potential risk of predation, and developed relatively stable strategies to deal with such conditions [@V2XKZN75#Wosniack_Etal_2017]. Thus, foraging behavior seeks to generate a strategy to maximize energetic intake in a partially known environment. However, if environment resources are non-depleting, it can lead to behaviors such as binge eating, finally resulting in excess caloric intake [@FIT9A4HW#Barson_2020].

To provide a connection between food-seeking behavior and uncertainty, evidence on the effects of increasing such uncertainty on the proximal effect of food-seeking behavior, that is, food intake is necessary. In that regard, it was pointed out that, possibly because of survival mechanisms, environment uncertainty increased food intake and reduced energetic spending. Then, the sufficient functions to support such findings were discussed, emphasizing related structures and functions associated with each one. Obesity was associated with sharp delayed-discounting and ACC atrophy, which points towards a sub-optimal pairing between reward value assignments, given environment uncertainty levels. Also, the OH system role in foraging was discussed as a proximal cause of overfeeding. Together, this suggests that food-seeking behavior evolved to provide optimal decision-making strategies in uncertain and scarce environments. However, (1) when environment energetic density is high, such strategies would result in overfeeding, and (2) obesity in itself can impair homeostatic regulation by altering structures related to uncertainty and reward value processing. Previous points predict that underlying foraging mechanisms, in certain environments, can lead to obesity.

# Obesogenic environments

# Cafeteria diet and uncertainty

# The decision making problem in obesity

# Conclusions

# Figures

![The figure illustrate how reward prediction error is diminished, as along a series of trials (circles denote trials, and three gray dots a series of trials) the reward value is learn and the error (doted gray lines) goes down. However when reward values are changed (last row) the reward prediction error goes up as this was not expected\label{rpe}](/home/nicoluarte/uni/PHD/UI/rpe.png)



# References
