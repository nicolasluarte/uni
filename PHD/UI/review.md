---
author: Luis Nicolás Luarte Rodríguez
title: Obesity and environmental uncertainty
csl: /home/nicoluarte/apa.csl
geometry: "left=4cm,right=4cm,top=1cm,bottom=2cm"
linestretch: 1.5
---

# Introduction

[---]

# Food-seeking behavior and uncertainty

Uncertainty, as broader concept, implies having limited or partial knowledge about something. Here, uncertainty is going to be understood as the knowledge of how a given action, inside a particular environment, relates to outcomes, for example, how does a lever press relates to the delivery of food, pressing the lever always gives food?, or only sometimes. Therefore, uncertainty represents the doubt about getting a food reward, when the lever is pressed, such doubt can represented as a distribution over possible values (food or no-food) with an expected value, and a standard deviation to quantify the amount of uncertainty over the expected value. Note that standard deviation is just one among other metrics of uncertainty [@UKGJNWU7#Preuschoff_Bossaerts_Quartz_2006].

When considering agency in decision-making tasks, uncertainty, refers to the incomplete information about the outcome of a given decision, and also incomplete information about the probability distribution governing such outcome, whereas 'risk' implies knowledge about such probability distribution [@AF2655I2#DeGroot_Thurik_2018]. A particular feature of risk is the inverted 'U' shape of its distribution. Where at outcome probability 0 or 1 is at minima, and at maxima when probability is 0.5, this derives from risk being measured as outcome variance between expected reward and actual reward [@UKGJNWU7#Preuschoff_Bossaerts_Quartz_2006]. If an animal is situated in a natural environment, it is unlikely to have complete information about the action-outcome pairing, and thus is forced to generate estimates of such pairing, or to build more complex models about the environment.


## Energetic balance in uncertain environments

Foraging is one of the most relevant cases of decision-making in natural environments.
An animal foraging is equivalent to searching for resources in a partially known environment, with depleting resources, and the supposition that the agent seeks to maximize its resources in the most efficient possible way, while accounting for an unknown resource distribution [@ESYGCSLH#Charnov_1976]. Multiple theories on how an agent must decide to optimally allocate time to each resource patch [@IJSUH426#Wajnberg_Fauvergue_Pons_2000], the optimal path to take [@A5N6NMAB#Hills_Kalff_Wiener_2013; @M5RXPXSZ#Humphries_Sims_2014] and evolutionary roots of such strategies [@V2XKZN75#Wosniack_Etal_2017].However, how environmental variables, such as resource uncertainty, modulates agent decision-making have been less reviewed. Likely due to a lack of integration over the fields of neuroscience, economics and psychology [@KPZRMRFZ#Rangel_Camerer_Montague_2008].

[TAG: foraging in uncertain environments; locomotor activity]
If an animal does not have perfect information about its environment, predicting if there is going to be food available in the near future turns into a problem. To avoid such problem, animals can consider how the current food availability fluctuates over time. If fluctuations are big, it indicates high uncertainty over food availability, if small, food availability is certain. How animals could make use of such information? In natural environments, contrary to experimental setups, uncertainty in food availability is probably a direct product of food scarcity. The probability of having a food encounter is reduced proportional to resource levels, up to the point that the levels are so low, that the certainty about resource depletion. Thus, is to be expected that uncertainty effects are in line with food scarcity signaling, that is, a expected reduction in energy expenditure in order to preserve the energetic balance. However, empirical evidence is not so clear in this regard [@U2XEEC7Q#Polo_2002], as both increases and reductions in body mass given increased levels of uncertainty in food availability has been observed [@CFC7DN65#Fokidis_Etal_2012], and this, might be related to increased locomotor activity [@B3IPWFXL#Ferretti_Etal_2019]. When controlling for total food intake in condition of fixed or variable food availability, birds show decrease their body mass, partly explained by increased locomotor activity [@CFC7DN65#Fokidis_Etal_2012]. Such increase in locomotor activity can be a product of increased foraging bouts [@8RIQXAVT#Lohmus_Sundstrom_Moore_2006], or maintaining the amount of foraging bouts, but making them longer [@PN23NXIS#Bordier_Etal_2018]. Additionally, mathematical modelling of foraging behavior, shows that an optimal strategy is to change foraging bouts to the start of the day, probably, in order to account for non-successful foraging bouts [@LLLWQCZE#Bednekoff_Houston_1994], which has also been observed empirically [@B3IPWFXL#Ferretti_Etal_2019]. These results can be interpreted as stress being a signal of food availability, and the modulation of foraging bouts responding to a decreasing efficiency of such bouts. Lower efficiency means that more bouts are to be made in order to sustain the energetic balance. While stress might signal uncertainty, the specific controller of foraging bouts, might be supported by the Locus Coeruleus activity, that disengages the animal from a given bout [@KA4Y29AG#Kane_Etal_2017].

[TAG: foraging in uncertain environments; why increase locomotor activity?]
As previously stated, uncertainty could signal food scarcity. Thus, a compensatory strategy is necessary in order to survive when such signal arrives. Intuitively, if food is scarce more effort should be on finding food, that is, an increase in food-seeking behavior is expected. Complementary to putting more effort in searching for food, a hoarding-type behavior might aid to cope with diminished resources. Both strategies are considered as mechanisms to prevent starvation in a near future, and are triggered by food-availability uncertainty, possibly because uncertainty is associated with food scarcity [@C6Z374UG#Anselme_Güntürkün_2019].

[TAG: foraging in uncertain environments; is it uncertainty or scarcity?]
Food scarcity can generate multiple changes, such as the number of foraging spots visited, diet diversity, and others [@G83L8BXA#Harris_Chapman_Monfort_2010], which are similar to the ones already discussed. Nevertheless, there exist some differences between food scarcity and uncertainty. A scenario were the caloric density is low but constant, exemplifies how an environment can have food scarcity but not uncertainty. Then, the question of what generates the previous behavioral changes arises. Evidence points that this effects are coming from uncertainty rather than scarcity, as changes regarding feeding routine, such as feeders positions, increases food intake compared to the constant environment [@NL4XYLRH#Forkman_1993]. @NL4XYLRH#Forkman_1993 study did not control for food-availability across conditions, however, in both conditions food was enough to satisfy energetic demands. To control for food-availability, equating total time available to get food has been used, and similar effects were reported when comparing predictable and unpredictable settings [@3GQMEPEH#Cuthill_2000].

[TAG: foraging in uncertain environments; resume]
Up to this point, there are at least 3 points to consider regarding food uncertainty, (1) under food availability uncertainty, food-seeking behavior is increased [@2YZE6SVW#Robinson_Etal_2014; @CFC7DN65#Fokidis_Etal_2012; @U2XEEC7Q#Polo_2002]. (2) when uncertainty is increased, strategies to maintain energetic balance, such as hoarding [@C6Z374UG#Anselme_Güntürkün_2019] or increasing body mass [@BW3RY7GD#Moiron_Mathot_Dingemanse_2018; @3GQMEPEH#Cuthill_2000] emerge. (3) such strategies imply a trade-off between preventing starvation and increasing risk of predation (due to reduced mobility), and as such are suggestive of a dynamic balance between increasing body mass to prevent starvation, and not doing so to retain mobility [@8XA5RHVI#Macleod_Etal_2005].

## Uncertainty increases food-seeking behavior

[TAG: food-seeking behavior in uncertainty; conceptual definitions]
To more precisely address behaviors associated with uncertainty, what happens around the food-reward acquisition must be considered. That is, what are the behaviors that are typical when food availability is uncertain?. Such behaviors, in experimental settings, are considered as the interactions with cues or apparatuses that are related to reward delivery. When the action is interacting with a conditioned stimuli (such as a lever or light), is called sign-tracking, whereas if interaction is with the food dispenser (or equivalent) is called goal-tracking [@ZW878GVH#Silva_Silva_Pear_1992]. More specifically, sign-tracking, refers to an approaching behavior towards previously conditioned stimuli and rewards. So, it implies a previous conditional-stimulus and unconditional-stimulus pairing, and, an afterwards tracking of the signal that was previously associated with the reward [@BDFISDRS#Flagel_2014]. This distinction is relevant because signal-tracking has been shown to respond robustly to uncertainty in food availability [@S8CHV5KG#Anselme_Robinson_Berridge_2013].

[TAG: food-seekign behavior in uncertainty; precisely defining an uncertain environment]
When uncertainty is introduced at the stage of conditional and unconditional stimulus pairing, as the probability of reward delivery upon lever pressing, sign-tracking increases as the probabilities of reward delivery approaches 50%, and the amount of reward is more varied [@S8CHV5KG#Anselme_Robinson_Berridge_2013]. In this case, as the delivery of a given reward gives no information about following one (delivery is determined by a probability function, independent of animal action), it can be assumed that, under Shannon entropy formulation, entropy (which can be understood as a measure of uncertainty) [@X596XYG3#Namdari_Li_2019] reaches the peak at 50% probability, and, furthermore, it predicts that uniform distributions, with more outcomes, increase uncertainty. Both were the case in the previously presented experiment (assuming uncertainty drove signal-tracking) as 50% probability of delivering 2 or 0 pellets had lower signal-tracking than 50% probability of delivering 0 or 1, 2, or 3 pellets with equal probability (16.7% for 1, 2 or 3 pellets). This, again, points out that increased food-seeking related behavior increases upon increased uncertainty even when food availability is controlled. This effect has been replicated in studies with amphetamine sensitization, where uncertainty (on conditioned stimulus and unconditioned stimulus) and sensitization, independently, augmented sign-tracking behavior, however, the effect of both uncertainty and sensitization was not additive suggesting a ceiling effect [@UZET5L4Z#Robinson_Etal_2015].

The increased, food-seeking related behavior magnification by uncertainty, has been found with partial reinforcement procedures [@P3MLPUGY#Collins_Etal_1983], with manipulation of food placement variability [@NL4XYLRH#Forkman_1993], variability on reward quality and delivery delay [@YI6J6Y7I#Craft_2016], and in sequential probability tasks [@YB2MIP4H#Stagner_Zentall_2010]. Implying a robust effect across multiple food-related uncertainty scenarios.


# Assessing and dealing with uncertainty

From the perspective of a foraging animal, food sources are distributed in a partially known space, where effort must be made to obtain such resources. Uncertainty, reveals the consistency of food sources in a given space and time, where more uncertainty determines more difficulty in estimating current food availability. However, the consistency of food sources must be sensed through a mechanism that updates its estimates in a trial by trial basis, because is safe to assume that an agent interested in sensing environment uncertainty does not possess complete information. A plausible mechanism is to sense uncertainty, indirectly, via the reward prediction error [@6ZEIWIGL#Colombo_2014]. The reward prediction error is simply $$ actual\,reward - expected\,reward $$ As the reward prediction error is thought to operate in environments where a particular action leads to a probable reward, this error is used to update the value of any given action, then, the value at such time step (which can be associated with a given action) is given by the discounted rewards from that point onwards up to the termination of the trial series [@2BEHEM7X#Sutton_Barto_2018] $$ expected\,reward = reward_{t+1} + \gamma reward_{t+2} + \gamma^2 reward_{t+3} + \ldots + \gamma^k reward_{T}$$ Here the trial series is composed of $T$ time steps with a discount factor $\gamma, 0 \leq \gamma \leq 1$. The discount factor is there to signal the typical preference for obtaining rewards now rather than latter, on and how big it is will depend on properties of both agent and environment [@R3TZXYBW#Glimcher_2011].

The formulation presented above is just a mathematical representation of several assumptions on how an agent can learn expected reward values in a finite, trial based experiment, and then calculate the prediction error at each time step. How this reward prediction error is used to update values will be presented latter on. However, the main idea is that, over trials, as the expected value approximates the real one, the reward prediction error goes down, nevertheless, if rewards value change, the error goes up reflecting such change (see figure \ref{rpe}).

The computation of reward predictions is dependent on the activity of the dopamine system [@8B5TPXB5#Schultz_2016]. As the reward prediction error was first derived from behavioral data, to assess the biological feasibility of error computation, three components must exist (1) expectation encoding units; (2) reward encoding units and (3) a subtraction unit [@NLDHLRVN#WatabeUchida_Eshel_Uchida_2017]

Reward prediction errors models predict three cases: (1) where the expected reward and current reward are equal (no prediction error); (2) expected reward is less than the current reward (negative error) or (3) expected reward is greater than current reward (positive error). Midbrain dopamine neurons have been found to encode positive errors but not negative under reinforcement learning models [@ZHGB75KH#Bayer_Glimcher_2005]. Around this point two main hypothesis have been formulated, the first, proposes than negative errors are encoded via lowering the fire-rate compared to the baseline [@33K2X73I#Schultz_Dayan_Montague_1997]. Whereas the second, proposes an opponency system between dopamine and serotonin systems [@JZJAAAYD#Daw_Kakade_Dayan_2002]. Furthermore, dopamine neurons, in the ventral tegmental area, have been found to encode the future discounted rewards [@JPZD7WDQ#Enomoto_Etal_2011]. This two lines of evidence points that dopamine is capable of encoding expectation, reward value and doing subtraction (perhaps including the serotonin system), showing a significant complexity of this system, which might exceed value-related computations [@698KWSAL#Takahashi_Etal_2017].

Above, the general function of dopamine neurons in reward prediction error has been stated, more specifically, this function seems to be related to the phasic activations, whereas, more sustained activation is related to reward uncertainty (measured as reward variance, thus reaching its peak at a probability of 0.5) [@AR2TQB84#Fiorillo_2003]. Such uncertainty-related signal has also been found in the orbito frontal cortex, amygdala [@YWWID6GW#Schultz_Etal_2008] and medial frontal lobe [@ZRTDBMRA#Huettel_2005]. A plausible hypothesis to link the reward prediction error and uncertainty encoding, can state that, over time, reward prediction error signals are integrated into an uncertainty signal, as, over time, more error is to be expected under higher reward variability (see figure \ref{uncertainty}). However, evidence points towards independent signals of reward prediction error and uncertainty in the orbito frontal cortex [@HVH8KZZ2#Rushworth_Behrens_2008]. Nevertheless, at least at a computational level, the reward prediction error can be used to estimate the reward-related uncertainty [@P2FYNJKR#Soltani_Izquierdo_2019].

[TAG: prediction error and learning rates, how are these linked?]
Prediction error is linked with uncertainty, as the error is expected to increase with higher uncertainty levels. Thus, prediction error allows to asses the value of actions and stability in an environment. To do this, only considering the absolute value of the error, is not enough. While the prediction error provides a measure of the discrepancy between our prediction and actual values, the learning rate determines in what magnitude such error influences our estimates. If the environment is completely uncertain, past information is irrelevant, and recent information should hold more importance, to reflect this learning rates should be high, so recent prediction errors influence the estimate to a greater measure, whereas if the environment is certain, learning rates should be lower to represent past information [@MYWY9LWC#Wu_Etal_2017]. Considering the development over the course of a task, at the start, higher learning rates are to be expected so learning happens at faster rate, however, as the task is properly learned learning rates should go down, so to not be influenced by random fluctuations [@89DIP32B#EvenDar_Mansour_2001].

[TAG: prediction error -> uncertainty -> learning rate]
Mathematical models, based on previously presented dopamine research, have proposed that learning rates are to be updated via the covariance between predictions (expected rewards) and prediction errors [@3B7ZWETH#Preuschoff_Bossaerts_2007], in the same line, empirical experiments have shown that humans behave according to a reward standard deviation-dependent scaling of reward prediction error [@CMAIA6R7#Diederen_Schultz_2015], so error should be less impactful when standard deviation is high. This proposed learning rate modifications are in line with the original model by @NZFTTQJZ#Pearce_Hall_1980, which proposed that 'surprise' affected the learning rate, or viewed from the other side, as the pairing between unconditioned stimulus and conditioned stimulus became more predictable, the 'associability' decreased. Learning rate increases as prediction error magnitude increases [@LIRXSMKQ#Jepma_Etal_2016], this allows for behavioral flexibility, as large errors signal environmental changes, and increasing the learning rates allow to maximize the importance of newer information. Is important to note that other systems, aside from dopamine, are able to track environment uncertainty level, such as the endocrine system through the stress response, measured as subjetive stress, pupil diameter and skin conductance [@WLQDZ5HW#DeBerker_Etal_2016], making uncertainty tracking via prediction error a possibility among others. 

Changing the learning rate based on the reward prediction error, reflects a constant tension an agent, faced with an uncertain environment, must face. How new acquired information must be considered?, a notion to answer this question is that of expected and unexpected uncertainty [@W3DEZPKU#Yu_Dayan_2005]. Expected uncertainty is the variability attributable to the stochastic nature of the reward, whereas unexpected uncertainty assumes that the agent is creating belief about action-rewards associations, and incoming information breaks such beliefs [@9Z525EYW#PayzanLenestour_Etal_2013]. Unexpected uncertainty, thus, has been proposed as top-down process which might be tracked by the Locus Coeruleus norepinephrine activity [@X6X8T8QT#Filipowicz_Etal_2020; @9Z525EYW#PayzanLenestour_Etal_2013]. Moreover, such activity, measured as pupil diameter, has been found to track the learning rates [@U2GSF8HI#Nassar_Etal_2012], which represents the end product of assessing environment in terms of expected and unexpected uncertainty. The intuition here is that when the statistical properties of an environment are changed, this should generate a signal of unexpected uncertainty similar to surprise as proposed by @NZFTTQJZ#Pearce_Hall_1980, which in turn represent that the current model of the environment must change in order to accurately depict it. Then, the learning rate must increase, so to give more weight to more recent information and correctly update the environment model [@Q3Q977MQ#Faraji_Preuschoff_Gerstner_2017].

Up until now, the discussion presented has focused on the calculation of reward value, typically, given a certain action. However, the computation of action-reward value is not directly linked to action choice. Picture a situation where the environment present high levels of uncertainty, and one action, until the present time step, have been associated with high rewards. If we were to choose just based on the maximum reward in such environment, we could miss potential better options, which true value, cannot be appropriately calculated because of variability. Such situation is more specifically defined by the exploration/exploitation dilemma, which posits that an agent, in order to obtain rewards, must 'exploit' current knowledge. However, it also must 'explore' to determine the best option in the future [@2BEHEM7X#Sutton_Barto_2018]. One of the findings is that the manipulation of dopamine levels modulates striatal representation of reward prediction errors, and subjects treated with L-dopa (a metabolic precursor of dopamine) chose, with more frequency, the option with greater reward compared to the placebo group and haloperidol (dopamine receptor antagonist) group [@87WR6HV3#Pessiglione_Etal_2006]. Although the authors did not report the temperature parameter (the one that determines the balance between exploration and exploitation) of the model, given that in all three group the optimal option was learned, it can be interpreted that the increase in dopamine levels effectively induced a bias toward exploitation. Direct evidence on the effect of L-dopa in exploration/explotation parameters, effectively shows that exploration is reduced, and this is associated with modulation of uncertainty signals in the insula and anterior cingulate cortex [@8FIHXB6G#Chakroun_Etal_2020].


## Assessing uncertainty in the present and future

Although the factors determining obesity as an outcome are multiple [@UQ9I8YFE#Ang_Etal_2013], it is reasonable to assume that the more immediate cause is excess intake relative to energetic demands. Moreover, excess intake is determined in an instance to instance basis, where a decision considering short and long-term benefits/risks must be made. With this in consideration, one can assume that obesity, in part, is caused by sub-optimal short/long-term benefit/risk assessments when making feeding decisions. If this was the case, areas that are related to computing options value in the short/long term, such as the ACC, should be in some way impaired.

Delayed discounting refers to the depreciation of a certain reward as a function of the time required to obtain it [@9YRMQU6H#DaMatta_Gonçalves_Bizarro_2012]. As such, it provides measures of how reward-related systems bias decision to the short or long term. Obese subjects show a robust tendency to steeply discount future rewards [@FCVQ7SB6#Amlung_Etal_2016], thus, favoring short-term rewards. 

Furthermore, ACC, among other structures, shows relative atrophy in obese subjects [@THS94923#Wang_Etal_2017;@5FBDWF55#Raji_Etal_2009], suggesting an impairment of the previously mentioned functions. These findings can be interpreted as if impairment in environment uncertainty assessment results in a preference for short-term rewards. If this were the case, palatable food sensory cues, which trigger food-intake, would dominate over more long-term modulated decisions, such as healthy food intake [@J37T9F5T#Higgs_2016].

Higher future rewards discounting paired with increased motivation to work for food, predict higher caloric intake [@GHWUZPNL#Rollins_Dearing_Epstein_2010], and this effect seems to hold even for low energy-density food [@TLRSYNN6#Epstein_Etal_2014]. The rate of reward discounting, thus, informs about the predisposition to increased energetic intake, independent of possible food-property related effects. Similar effects have been found in children [@8QS2B4AF#Best_Etal_2012], but not in adult males [@IATULRKH#Smulders_Boswell_Henderson_2019]. Moreover, these effects seem to be directly related to body fat [@7B4XUYMB#Rasmussen_Lawyer_Reilly_2010].


## The bias towards immediate rewards

Previously, evidence on how uncertainty modulated feeding behavior, in terms of decisions between known and unknown options, and immediate and future valuation was presented. What follows aims to examine previous evidence in the case of overfeeding, specifically in obesity.

Temporal-difference learning models state how agents can estimate reward values in uncertain environments. At each time-step, the agent computes the value of a given state considering: (1) the estimated value (randomly initiated at first), and (2) the temporal-difference error, which represents the distance between the estimate of state value and the actual reward obtained in such state.

\begin{equation}
	V(S_t) \leftarrow V(S_t) + \alpha(Temporal \, Difference \, Error)
\end{equation}

$V(S_t)$ denotes the estimated value at a given state, and $\alpha$ is used to model the agent learning rate. Additional parameter $\rho$ has been proposed to model sensitivity to reward [@JGKMEKV6#Huys_Etal_2013; @GU4KJGAS#Kroemer_Small_2016], such that the temporal difference error accounts for the subjective value of obtained rewards.

\begin{equation}
	Temporal \, Difference \, Error = \rho \times Reward - V(S_t)
\end{equation}

Obese subjects had shown reduced dorsal striatum activity to food rewards, which has been interpreted as reduced pleasure for food. However, simulations under the previously presented model show another option. That is, obese subjects show heightened reward sensitivity but decreased learning rates, ending in a lowered state value estimation [@GU4KJGAS#Kroemer_Small_2016]. Modeled learning rates measures had shown that this is the case in obese subjects. Moreover, it points that negative prediction errors (the equivalent of temporal difference error) were used to a lesser extent than lean subjects, whereas positive errors showed no differences [@DBGUDZRL#Mathar_Etal_2017]. This can be interpreted as a difficulty to update reward or state values when the estimated reward is higher than the actual reward, possibly reflecting a short-term reward estimation.

It should be noted that more recent neuroimaging evidence points in favor of a hyper-reactivity of rewards circuitry, instead of hypo-reactivity. However, conclusions obtained by the model still hold, as such, hyper-reactivity is accompanied by a bias towards immediate rewards [@WCFW3TEU#Stice_Burger_2019]. In line with the reinforcement learning model presented, evidence from probabilistic learning paradigms in obese subjects shows a decreased impact of negatively valued choices on consequent behavioral adaptation [@PE46B3Z8#Kube_Etal_2018]. These seemingly opposing results can stem from, previously not considered, quadratic associations between BMI (body mass index) and reward sensitivity, where an inverted U-shape is observed as BMI increases [@67GSY6VY#Horstmann_Fenske_Hankir_2015]. Taken together, this finding suggests that obesity overfeeding is not only reliant on increased reward sensitivity (more reward sensitivity is assumed to increase intake), but other parameters such as learning rates can determine the overall valuation of the reward, biasing decision-making to immediate rewards, that paired with highly palatable food can lead to excess caloric intake. This, because, while palatable food definition is not standardized [@NBA5EBE8#Fazzino_Rohde_Sullivan_2019], it can be assumed that they, typically, consist of high caloric density. However, there might be additional effects of palatable foods in decreasing taste sensitivity related brain areas, which in turn, might favor further intake [@LFEELSI4#Yokum_Stice_2019].


# Taking action in uncertain environments

Previously, the notion that there is something linking the estimated values and the actions taken was presented in terms of exploration/exploitation. How an agent decides, based on its estimated, to behave at any given time is called a 'policy', and as such, it constitutes a mapping from estimates and actions [@2BEHEM7X#Sutton_Barto_2018]. As presented previously, the reward prediction error representation is able to guide the chosen policy of the agent [@87WR6HV3#Pessiglione_Etal_2006]. Heuristics, which are strategies that rely heavily on exploiting environment statistical properties, have been proposed to be guiding decision-making in uncertain environments [@J7XQF8LY#Hafenbrädl_Etal_2016]. Some heuristics are thought to be an evolutionary derivative from uncertain environments [@S53QRP94#VanDenBerg_Wenseleers_2018]. However, what aspect of uncertainty is the one used to selected the optimal policy is not clear [@LFIY67XG#Gershman_2018]. @LFIY67XG#Gershman_2018 explored the fit of two models to human behavioral data in two-armed bandit tasks, and found signatures of both models in behavioral data, while a mixture of both models more salient signatures represented the best fit. The first model fit corresponded to Upper-Confidence-Bound [@NXA58KQ4#Auer_CesaBianchi_Fischer_2002], the intuition is that an agent should choose based on the times a certain action has been taken, and the potential value of each action on the environment. The action selection is formally assigned as:  $$A_t := \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c \sqrt{ \frac{ln_t}{N_t(a)} } \right] $$ Here $Q_t(a)$ represent the expected value of taking action $a$, in $c \sqrt{ \frac{ln_t}{N_t(a)} }$ the denominator represents how many times action $a$ has been chosen up to a certain point $t$, as time $ln_t$ appears in the numerator, when action $a$ is not chosen its upper-bound will increase, but decrease if such is continously chosen. Note that @LFIY67XG#Gershman_2018 used a modified version of this algorithm to reflect human decision stochasticity, nevertheless, this provides enough insight into how it considers uncertainty. Author found indirect support for this policy, by considering that reaction times are faster when estimated rewards are more different [@2V3MQIX5#Tajima_Drugowitsch_Pouget_2016], and that reaction time decreased in proportion to increasing relative uncertainty, thus acting according to an uncertainty bonus as posed from the Upper-Confidence-Bound. The second model examined corresponded to Thompson Sampling, which builds reward priors on each option, this priors are beta-distributed with parameters $\alpha$ and $\beta$ according to: $$p(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}, \, \theta \in [0, 1]$$ Where $\theta$ is the model expected reward, and $\Gamma$ represents the Gamma function.To illustrate how the priors are updated a win/loss reward environment can be considered. First, the agent will sample from each of the distribution, and will choose the action associated with the distribution that gave the largest sample. Then, if the reward is a win (or '1') the $\alpha$ is updated as $\alpha = \alpha + 1$ and $\beta = \beta$, when the observed reward is a loss (or '0') $\beta = \beta + 1$ and $\alpha = \alpha$. Findings by @LFIY67XG#Gershman_2018 noted that signatures corresponding to this model were that choice stochasticity (exploration actions) were proportional with the level of uncertainty in the model distributions.

Evidence for directed exploration, based on uncertainty levels, has been found in humans [@AFIWCGVS#Blanco_Sloutsky_2019; @VRHNLHDG#Wilson_Etal_2014]. However, while directed exploration seems to be a robust strategy in humans, certain aspects emerge and vary throughout the life span [@26M86HJZ#Somerville_Etal_2017], pointing towards a complex and dynamic system. The main idea behind the previously presented models are twofold (1) uncertainty, in someway, guides the balance between exploration and exploitation and (2) simple computations can sufficiently describe exploratory behavior under varying levels of uncertainty. Integrating over evidence of foraging under uncertainty and computational models presented, food-seeking behavior can be stated as a series of actions, occurring in an uncertain environment, where each action (feeding bout) is evaluated in terms of the reward prediction error. Reward prediction error, however, not only informs about the value of the expected and current reward, but also, by considering the history of such errors, calculates environmental uncertainty [@BUXRNM89#Mikhael_Bogacz_2016] (see figure \ref{uncertain_agent}) and represents it at a neural level [@AR2TQB84#Fiorillo_2003]. Finally, action policies are intimately related to uncertainty, thus establishing a clear link between feeding-related behavior and environment uncertainty.


## Uncertainty representations at the neural level

If uncertainty can modulate food-seeking behavior in order to increase intake and better sustain energetic reserves, it is expected to have at least to functional instances (1) an uncertainty sensing unit and (2) a reward processing unit, which can relay information to homeostatic-related and decision-making loci, to integrate such information and determine the next action to take. To determine the neural substrates of such instances, environment-agent dynamics can be represented through Markov decision tasks. Such tasks consider a set of states with possible transitions between each one, and two functions: (1) the one in charge of determining the state transition given the agent action and (2) an action-state-reward function, which maps a reward to a given action-state tuple. In such tasks, uncertainty is derived from probability matrices assigned to either of the two functions. When state transition functions are manipulated, two scenarios can be created: (1) a regular one, where action-state transitions are deterministic, and (2) a random one, where action-state can not be predicted. 

A plausible neuronal substrate underlying such functions is the striatum, as it has been found to be involved in decision-making actions such as action selection [@6JBBK7KQ#Balleine_Delgado_Hikosaka_2007], action value representation [@5CLQA59J#Samejima_2005], and reward representation [@UVXRU55G#Wake_Izuma_2017]. Markov decision tasks can be used to test behavior under certain or uncertain environments. Such task comprises of a set of states, which are accessible by performing certain actions, however, the function that defines the mapping between actions and states can be random or deterministic. Finally, entering a given state provides a reward. When comparing learning of certain versus uncertain Markov decision tasks, the dorsal striatum seems to be more associated with the uncertain condition, whereas the dorso lateral prefrontal cortex showed greater activation under the certain condition [@G5PZHPCI#Tanaka_Etal_2006]. This can be explained in terms of immediate and long-term reward prediction, as state transitions are more uncertain, subjects can only reliably predict the following rewards, in turn, if state-transition dynamics are deterministic, the reward over a long series of actions can be predicted, thus, making useful to consider rewards in the long term. A similar relationship between uncertainty and immediate/long-term rewards has been noted in human consumers when exposed to features typically associated with environmental uncertainty, such as, economic crisis, unemployment, among others [@S53QRP94#VanDenBerg_Wenseleers_2018].

If an environment is stable, then state-action-reward mappings can be optimized to reduce reward-prediction error. In this way, when the mapping is optimized, reward-related circuitry should reduce its activity [@V9CIKRBE#Friston_2009]. However, this mapping is always modulated by environment dynamics regarding uncertainty. An optimal mapping in a given environment state can increase the reward prediction error in the same environment if this is non-stationary. The anterior cingulate cortex (ACC) has been shown to increase its activation levels when predictability in the environment drops [@9SRBHI4W#Davis_Choi_Benoit_2010], effectively signaling environment dynamics.

As previously stated, environment dynamics need to be taken into account in order to appropriately interpret obtained rewards. If I visit a restaurant and the food served is delicious, my rating of the restaurant should not be too hasty as this could be just good luck. However, if this has always been the case, giving a high rating would be the correct choice. In term of rewards, uncertainty is high when a given rewards give no information about the ones to come, conversely, certainty is achieved when a given reward gives all information about the following one. Direct tracking of environment volatility has been found to be well represented in the ACC [@BHR2NAEI#Behrens_Etal_2007], presumably by encoding some sort of learning rate that bias valuation of rewards more to the short-term if volatility is high, and to the long-term is volatility is lower. The competing hypothesis of ACC describes its function to a decision-difficulty sensing unit, or demand of control when overriding default action is more optimal [@J9QC5JYH#Shenhav_Cohen_Botvinick_2016]. However, it should be noted that @BHR2NAEI#Behrens_Etal_2007 results were circumscribed to the time point where the outcome is observed, which corresponds to the proper timing to assign obtained reward influence to the following behavior.

When representing the uncertainty of a given environment, an agent must pair the value obtained with the action performed. For each action possible, the agent updates the value of the action-reward tuple based on the reward prediction error.

Temporal dynamics of action-reward pairing and reward prediction error are such that the former occurs first relative to the later. Such temporal difference is reasonable because the pairing should be represented when taken action, and the prediction error requires feedback in order to compare obtained versus expected rewards. Considering this, the action-reward pairing has been found to be correlated to activity at the putamen, whereas rewards-prediction error, to be represented in the caudate nucleus [@DAKDZEVY#Haruno_Kawato_2006]. However, as the authors point, both structures are likely to be involved in a larger loop containing the ACC, which would make sense to integrate reward evaluation over states, actions, and environmental uncertainty, and optimally influence following behavior.

It can be inferred from the way action-reward pairing is stated that it corresponds to action selection based on a history of rewards, which are mediated by the reward prediction error. Inhibition of putamen activity has effectively shown a reduction in performance when the task requires the consideration of reward history to select correct actions [@XZBUNRIS#Muranishi_Etal_2011]. Signal encoding, however, seems to be more complex, as basal ganglia direct pathway encode rewards outcomes, and the indirect pathway represents the next-action selection [@4F2PUL7U#Nonomura_Etal_2018]. Together, this points to a multi-structure network that represents expected and obtained rewards as an error, which allows easing computational requirements as the current state needs only to be compared with the expectation, that encompasses all previous history of rewards. Moreover, this signal updates rewards given actions, while considering environment volatility and the proper weighting of immediate versus long-term rewards. Thus, allowing to optimize behavior even when environments are non-stationary and rapidly changing.


### Orexin role in reward function

Orexin is a neuropeptide with two isoforms, orexin-A and orexin-B, mainly associated with regulation of arousal and feeding behaviors [@PQWIJHAP#Siegel_2004]. While orexin neurons are mainly localized in the lateral hypothalamus, they project widely throughout the brain [@C4QSBWHV#Nambu_Etal_1999; @TUU47UNG#Harris_AstonJones_2006; @PQWIJHAP#Siegel_2004]. Lateral hypothalamus and orexin have been related to feeding and reward-related behavior [@2HED3PSJ#Dileone_Georgescu_Nestler_2003, @S6TNU7XM#Harris_Wimmer_AstonJones_2005]. Lateral hypothalamus regulates feed behavior [@MW2YHV9D#Margules_Olds_1962; @QWCCGZTT#Sweeney_Yang_2016; @LJVQFQ6K#Anand_Brobeck_1951], and does this by controlling orexin activity, where increased orexin activity increases feeding [@QFWKWDA5#Ardianto_Etal_2016; @TPGJBDDD#Kotz_Etal_2002]. Furthermore, orexin-related arousal increase is modulated by extracellular glucose concentrations, and neuroendocrine markers of energetic balance[@KPIHYUYF#Yamanaka_Etal_2003], pointing to the inclusion of 'contextual' information to its function. On the other hand, the reward-related learning and valution seem to be in charge of the dopamine system, and significantly sustained by the activity in the ventral tegmental area [@8TKQDEML#D'Ardenne_Etal_2008; @R775Y2Q6#Eshel_Etal_2016], that uses the reward prediction error to promote task learning [@E6MZ8HIA#Keiflin_Etal_2019]. While, ventral tegmental area, is involved in the encoding of more that prediction errors, this might be more complex as to include current beliefs about the environment [@SZZJ3VQF#Gershman_2017]. Here, how 'contextual' information, such as environment uncertainty and animal current energetic balance, can affect how prediction errors and learning is assessed.

To the lateral hypothalamus to provide 'contextual' information to the ventral tegmental area, would mean that (1) lateral hypothalus encodes contextual information, (2) can rely this input to the ventral tegmental area, and (3) the input from the lateral hypothalamus modulates the excitability of dopamine neuron in the ventral tegmental area, thus altering the prediction error signal.


#### Lateral hypothalamus encodes contextual information

The signal carried by the lateral hypothalamus is complex as it encodes, among others, reward predictability and reward uncertainty [@V2PX8TRL#Noritake_Nakamura_2019]. This 'context' signal from the lateral hypothalamus to the ventral tegmental area, could explain why patients with narcolepsy-cataplexy (caused by the loss of orexin neurons of the lateral hypothalamus) show decreased performance in decision task with uncertainty but not in normal decision tasks, while preserving reward sensitivity [@BGNECK7I#Bayard_Etal_2011]. The latter is in line with the interpretation of lateral hypothalamus function as a 'predictive' homeostasis controller, feeding contextual information into the brain and signaling multiple control signals, while having physiological state feedback [@IQWZPMP7#Burdakov_2019]. Such function of lateral hypothalamus is well supported by its anatomical disposition [@SC7QH8NN#Geisler_Zahm_2005].


#### Lateral hypothalamus relies information to the ventral tegmental area

Lateral hypothalamus projects a dense network of axons to the ventral tegmental area dopamine neurons, and this modulates reward-related behavior via glutamatergic activity [@P8YAF6NZ#Kempadoo_Etal_2013. @S3LMSWG5#Geisler_Etal_2007], connectivity, with the same type of activity, exist between the lateral hypothalamus and lateral habenula, which may exert indirect control over the ventral tegmental area [@SZMQLQTN#Stamatakis_Etal_2016]. In addition to glutamatergic activity, those projections also contain a GABAergics element, which is activated by leptin action [@GDVXZGJY#Leinninger_Etal_2009]. Among those innervations, a significant portion contains orexin neurons, and is mainly limited to the ventral tegmental area [@4PLXQQT9#Fadel_Deutch_2002]. These projections have an important functional role as they are involved in compulsive sucrose seeking [@HT5LXV4U#Nieh_Etal_2015], and more general behavioral activation throughout inhibitory GABAergics outputs from the lateral hypothalamus to the ventral tegmental area [@GZ3AZ6UU#Nieh_Etal_2016].

The lateral hypothalamus orexin activity inputs into the ventral tegemental area via orexin receptor type 1 [@KN9TLADJ#Richardson_AstonJones_2012], which allow an indirect modulation of the ventral tegemental area dopamine output to nucleus accumbens lateral shell, medial shell, and basolateral amygdala, effectively increasing activity in the first two, but not in the latter [@XIJMRM6S#Baimel_Etal_2017]. Thus, lateral hypothalamus orexin mediated activity might modulate reward-related behavior sustained by structures beyond the ventral tegmental area by influencing its output.

Previously how the hypothalamus encoded contextual information was considered, with recently presented information the possibility of such contextual information to be carried into the ventral tegemental area is open. Furthermore, it points that such information might modulate structures beyond the ventral tegemental area.


#### Lateral hypothalamus modulates the prediction error in the ventral tegmental area

Orexin activity increases the intake of palatable food when injected in the ventral tegmental area [@TESJ97B4#Terrill_Etal_2016; @Q8IHJK68#Mattar_Etal_2020], moreover, the orexin role in the lateral hypothalamus and ventral tegemental also modulated more general reward-related behavior [@S6TNU7XM#Harris_Wimmer_AstonJones_2005; @YMQYAEQ8#Mahmoudi_Etal_2020]. Furthermore, activation of the lateral hypothalamus neurons projecting to ventral tegmental area promotes reward seeking behavior, and displays a prediction error-like profile activity [@HT5LXV4U#Nieh_Etal_2015]. Lateral hypothalamus representing a prediction-error like activity, and its connectivity to the ventral tegemental area, prompts to think of a direct modulation of the dopamine prediction error.

Lateral hypothalamus stores, previously learned, stimulus-reward pairings [@4ESME7SG#Sharpe_Etal_2017; @V2PX8TRL#Noritake_Nakamura_2019], this can then inform about the expected value of a given action or stimulus in an previously experienced environment. If such information is not available the prediction error magnitude should increase as proper expectation is never properly formed. An interesting case is the one where stored expected values are withdrawn at the time of prediction error computation, but kept stored for the expected value update. @4ESME7SG#Sharpe_Etal_2017 observed this by optogenetically inactivating the lateral hypothalamus GABA neurons (which carry the expected value) terminals in the ventral tegmental area, only at cue presentation time in a typical pavlovian conditioning task, and found that mice spent more time in the food port upon positive conditioned stimulus. While counterintuitive, this case can be analogously understood as and increased learning rate, that is, having a larger prediction error (and thus increasing learning) can be obtained by increasing the learning rate or not considering the expected value in the computation, leading to a faster convergence to the estimated value. Because of the simple nature of the task this procedure lead to increased performance, whoever faster convergence can lead to suboptimal decision making in more complex scenarios [@89DIP32B#EvenDar_Mansour_2001]. This 'storage' of stimulus-value hypothalamic function is in line with evidence showing proportional orexin activity response to food or drug preference and drug extinguished preference reinstatement [@9MIJEAWP#AstonJones_Etal_2010], thus showing capabilities of encoding reward values and use them in a future time. This kind of signaling performed by the lateral hypothalamus, adds up to canonical role in feeding [@2ZT7CAFB#Jennings_Etal_2013; @V8LPCV6E#Delgado_Anand_1952] into a more complex connection to the reward systems, which, as previously mentioned, exceed the ventral tegmental area while sustaining the reward prediction error correlates, for example, in the nucleus accumbens [@7SLC53RQ#Werlen_Etal_2020].

For orexin to modulate the reward function, specifically the prediction error, a series of conditions were considered, as the capabilities of the lateral hypothalamus to encode contextual information, and effectively input that into the ventral tegemental area. Moreover, it was considered that such input should be capable of altering the prediction error signal computed in the ventral tegmental area. All such capabilities are sustained by anatomical and functional evidence, and as such posit the role of orexin in a pivotal role to the computation and update of rewards expected value. The main importance of assessing the previously mentioned requisites, is to add an important part to the presented models, that is, how environment information can be more directly taken into account at a neural level, which adds up to the neural representations of uncertainty purely regarding reward-related computations.


# The obesogenic environment

[@ELZ2PYYM#Mascia_Etal_2019]
[@DJFPYDKI#Hammond_Etal_2012]


# Conclusion

[---]

# Figures

![Figure shows a series of 10 trials, where from trial 0-4 the true reward value is 100, and for the remaining trial its 50. A very basic agent was simulated to update its estimates based on the reward prediction error. Initial estimates were set at 0. Notice how during trial 0-4 reward prediction errors are positive and decrease to 0, because the reward obtained was, initially, greater than the estimate. In contrast, in trial 5, when reward changes to 50, the error becomes negative because the estimate was near 100\label{rpe}](/home/nicoluarte/uni/PHD/UI/figure1.png)

\newpage 

![Simulated agent learning under two environments, (1) in gray, rewards are sampled from a normal distribution with mean = 100 and standard deviation = 10, (2) in black, mean = 100 and standard deviation = 1. Black and gray lines represent the reward prediction error rolling standard deviation over 100 trials. Notice how, over the trials, this 'signal' approximates to the underlying uncertainty of the distribution (using the standard deviation as measure of uncertainty).   \label{uncertainty}](/home/nicoluarte/uni/PHD/UI/figure2.png)

\newpage


# References
